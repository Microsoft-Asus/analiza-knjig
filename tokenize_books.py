import pandas as pd
import multiprocessing
import tools
from nltk import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import time

s_words = stopwords.words("english")
common_words = set([
    'the',
    'of',
    'to',
    'and',
    'a',
    'in',
    'is',
    'it',
    'you',
    'that',
    'he',
    'was',
    'for',
    'on',
    'are',
    'with',
    'as',
    'I',
    'his',
    'they',
    'be',
    'at',
    'one',
    'have',
    'this',
    'from',
    'or',
    'had',
    'by',
    'not',
    'word',
    'but',
    'what',
    'some',
    'we',
    'can',
    'out',
    'other',
    'were',
    'all',
    'there',
    'when',
    'up',
    'use',
    'your',
    'how',
    'said',
    'an',
    'each',
    'she',
    'which',
    'do',
    'their',
    'time',
    'if',
    'will',
    'way',
    'about',
    'many',
    'then',
    'them',
    'write',
    'would',
    'like',
    'so',
    'these',
    'her',
    'long',
    'make',
    'thing',
    'see',
    'him',
    'two',
    'has',
    'look',
    'more',
    'day',
    'could',
    'go',
    'come',
    'did',
    'number',
    'sound',
    'no',
    'most',
    'people',
    'my',
    'over',
    'know',
    'water',
    'than',
    'call',
    'first',
    'who',
    'may',
    'down',
    'side',
    'been',
    'now',
    'find',
    'any',
    'new',
    'work',
    'part',
    'take',
    'get',
    'place',
    'made',
    'live',
    'where',
    'after',
    'back',
    'little',
    'only',
    'round',
    'man',
    'year',
    'came',
    'show',
    'every',
    'good',
    'me',
    'give',
    'our',
    'under',
    'name',
    'very',
    'through',
    'just',
    'form',
    'sentence',
    'great',
    'think',
    'say',
    'help',
    'low',
    'line',
    'differ',
    'turn',
    'cause',
    'much',
    'mean',
    'before',
    'move',
    'right',
    'boy',
    'old',
    'too',
    'same',
    'tell',
    'does'
    'set',
    'three',
    'want',
    'air',
    'well',
    'also',
    'play',
    'small',
    'end',
    'put',
    'home',
    'read',
    'hand',
    'port',
    'large',
    'spell',
    'add',
    'even',
    'land',
    'here',
    'must',
    'big',
    'high',
    'such',
    'follow',
    'act',
    'why',
    'ask',
    'men',
    'change',
    'went',
    'light',
    'kind',
    'off',
    'need',
    'house',
    'picture',
    'try',
    'us',
    'again',
    'animal',
    'point',
    'mother',
    'world',
    'near',
    'build',
    'self',
    'earth',
    'father',
    'head',
    'stand',
    'own',
    'page',
    'should',
    'country',
    'found',
    'answer',
    'school',
    'grow',
    'study',
    'still',
    'learn',
    'plant',
    'cover',
    'food',
    'sun',
    'four',
    'between',
    'state',
    'keep',
    'eye',
    'never',
    'last',
    'let',
    'thought',
    'city',
    'tree',
    'cross',
    'farm',
    'hard',
    'start',
    'might',
    'story',
    'saw',
    'far',
    'sea',
    'draw',
    'left',
    'late',
    'upon',
    'away',
    'project', # because project gutenberg destroys this result
    'gutenberg',
    'la',
    'et',
    'il',
    'un',
    'ei',
    'que',
    'der',
    'ã',
    'à',
    'â',
    'und',
    'le',
    'se',
    'de',
    'les',
    'ja',
    'je',
    'ne',
    'se',
    'ce',
    'dans',
    'pas',
    'en',
    'des',
    'een'
])

def tokenize_book_to_words(text):
    sentances = sent_tokenize(text)
    return [word for sentance in sentances for word in map(str.lower, word_tokenize(sentance)) if word not in s_words and word.isalpha() and word not in common_words]


def t_book(index):
    book = tools.read_book(str(index), 'processed_data/books')
    n_book = tokenize_book_to_words(book)
    tools.write_array(n_book, str(index), "processed_data/tokenized_books")
    
if __name__ == '__main__':
    book_data = pd.read_csv('processed_data/book_data.csv', index_col = 'id', header=0)
    ids = book_data.index
    s_time = time.time()
    pool = multiprocessing.Pool(processes = 7)
    pool.map(t_book, ids)
    pool.close()
    print("This took {}s".format(time.time() - s_time))
