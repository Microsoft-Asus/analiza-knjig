from bs4 import BeautifulSoup
import tools
import re
import argparse

base_url = 'https://www.gutenberg.org'


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        '--exclude-categories',
        help=
        'Do not process categories (should be used if categories are already loaded',
        action='store_true')

    args = parser.parse_args()

    if not args.exclude_categories:
        process_categories()


def format_title(title):
    title = title.replace('(Bookshelf)', '')
    return title.strip()


def categories(html_file):
    '''Get all the categories from the category page.'''
    soup = BeautifulSoup(html_file, 'html.parser')
    cats = []
    # For every bookshelf (even though there should only be one) get all of its elements.
    for bookshelf in soup.find_all('div', 'mw-category'):
        for category in bookshelf.find_all('a'):
            title, link = category.get('title'), category.get('href')
            cats.append({'title': format_title(title), 'link': link})
    return cats


def join_url(base, rest):
    return base + '/' + rest


def process_categories():
    ''' Get all the categories belonging to fiction along with their respective links.'''
    category_url = 'Category:Fiction_Bookshelf'
    file_name = 'category_fiction'
    directory = 'pages'
    tools.download_page(join_url(base_url, category_url), file_name, directory)
    cats = tools.read_file(file_name, directory)
    cat_dicts = categories(cats)
    tools.write_csv(cat_dicts, ['title', 'link'], 'categories', '')


if __name__ == '__main__':
    main()
